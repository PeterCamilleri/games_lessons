# Appendix Z - Extra Credits

This section is only of interest to historian types and those who'd rather not
be doomed to repeating the mistakes of the past. Otherwise, this section can be
seen as a platform for a rant as well as yelling at kids to get off my lawn.

In ancient times, I recall the 1989 Charles Petzold book:
[Programming the OS/2 Presentation Manager](https://www.amazon.ca/Programming-Presentation-Manager-Applications-Environment/dp/1556151705/ref=sr_1_29?ie=UTF8&qid=1535567521&sr=8-29&keywords=charles+petzold).
In particular I recall the Hello Word program. If memory serves, it was a
massive chunk of 'C' code running about six pages long. It was also convoluted,
difficult to understand, and full of cryptic Presentation Manager API calls.
I have memories of struggling to put together applications using those same
programming tools and it was pure frustration. In case you think that this was
kust an OS/2 thing, Windows 3.1 was much worse and the Win32 API started in
Windows 95 did little to really help out the situation.

So the question before us is simply this: **Why was it so bad?** Let's identify
the key villains:

#### The 'C' language:

Way back in 1989, the 'C' programming language was the undisputed king of the
programming languages. While some primitive Luddites clung to the wretched
assembly language, most recognized that high level languages, like 'C',
promoted better results through improved code clarity, fewer code defects, and
enhanced code portability. That is, except when it didn't.

Windowed graphical user interfaces (GUI) were the one of the original use cases for
the entire concept of object oriented programming (OOP). On a basic level, a
GUI entity on the screen is an object that responds to input signals or
messages from the user and other entities. In OOP, objects are entities that
are defined by how they respond to inputs or messages.

Messages sent to a window or object, have no strictly ordained order. They
occur as needed or are generated by user activity.

In 'C' this is not the case. Here things are strictly ordered by the structure
of the code. Steps occur as laid out in the code. So much so that this approach
is called "structured programming". To create a windowed environment in 'C', one
must create a message handling emulator. The responsibility for creating and
maintaining this emulator falls squarely on the programmer. Time wasted here is
time not available for doing useful, application oriented work.

In case anyone should think that this is a diatribe against 'C', it is not. I
have used that language numerous times in many embedded systems projects where
resource constraints required a lean and compact solution and the task at hand
lent itself to a structured approach. 'C' was in its element.

Summary: Using 'C' in a GUI is akin to using scissors to mow the lawn. It can
be made to work; It's just not a good idea.

#### The 'C++' language:

Around the time that 'C', and the structured programming paradigm in general,
were running out of steam, a new programming language was gaining ascendancy.
That language, the brainchild of
[Bjarne Stroustrup](http://www.stroustrup.com/), was 'C++'. One of the guiding
principles of 'C++' was that it be a compatible enhancement of 'C'. In fact,
an early 'C++' compiler (called cfront I believe) actually compiled 'C++' code
into 'C' code. This compatibility was a major advantage for 'C++' as it made
the large amounts of 'C' code available as a starting point for any new
developments.

This feature was also a major problem as well. I recall hearing the following
(sorry I cannot recall where or who) "C++ is pretty good. Too bad about all the
C." 'C++' was also a much more restrictive object oriented language. For
example class types were determined statically at compile time rather than at
runtime. To get around these and other restrictions, complex facilities such
as multiple inheritance and templates were added. Implementation by the early
compilers of the day was incomplete, bloated, slow, and riddled with bugs.

Another problem was caused by the fact that the object oriented features all
were contained in constructs in the compiler and excluded from the running
programs. This has been compared to a ghost town. Entities run as expected, but
access to even the most basic details, like what class is this? or what is this
message? are not available. This made it very difficult to connect the message
passing system of 'C++' to the messages of the window manager of the operating
system. Most often this resulted in a huge 'switch' statement, just like in
plain old 'C'.

Trying to get around these problems, many non-standard language extensions were
proposed and implemented. These only added to the confusion and the fog of
proprietary obstruction.

Summary: For cutting the lawn, motorized scissors are not always better, and
can be a great deal more dangerous.

#### The Really Awful Hardware:

If memory serves, in the early 90s, I was involved in a project to create a
sophisticate GUI based application for OS/2. The language was NOT 'C'. It was
Smalltalk. Graphical User Interface were invented under that language by Xerox
in the 80s. It was the ideal language for such work.

So if the ugliness of 'C' was not a factor, why was the development such an
agonizing death march?

It did NOT help that the language was called "Smalltalk". Explaining to
grim-faced suits that you are embarking on a costly development process in
"Smalltalk" is never going to be easy. I seriously think that the name was a
factor in the failure of that language.

Most likely, though, it was the hardware.

The hardware! We were dealing sluggish Intel 386 CPUs, on motherboards that all
maxed out at a pitiful 16Meg of memory, and dumb video cards that forced the
CPU to do all the grunt work of computing each and every pixel drawn. To be
fair, in the day, these were considered to be good machines, for those with
finite budgets.

The fact that memory manufacturers were colluding to limit the supply of memory
devices and jack up prices probably did not help either.

Like Ruby, Smalltalk is an interpreted, dynamic language. Unlike Ruby, it was
stone age primitive. The virtual machine interpreter and memory management were
all many times less stable and efficient than those in Ruby today.

In short it was really annoyingly slow, paused a lot for garbage collection,
and crashed often enough to make users of the application want to go back to
the character mode, DOS based application we were trying to replace.

The industry was in a state of transition. These machines where over-powered as
MS-DOS machines. They were completely inadequate OS/2 (or Windows/NT) machines
for anything but the most trivial tasks written in, yes, 'C'.

Summary: If your lawnmower is a plastic children's toy, the scissors may be the
only way to cut the lawn.

#### The Evil Committee of Taking Over the World:

Look, capitalism requires that corporations seek greater profits, not the
greater good. This includes companies that market and sell operating systems
and graphical user environments. In the early 90s, this included Apple, IBM,
Microsoft, and Novell. Each wanted to maximize the value of their offerings
over those of their competitors.

That meant that there was an incentive to "lock-in" applications to their
platform and to make portable code as difficult as possible.

So how did this legally mandated greed make life miserable for application
developers?

For starters, graphical user interfaces were designed to be as different and
incompatible as possible. This often meant that they were far more complex
and convoluted than necessary.

Yoda Summary: Complexity &#8594; Pain.

In addition, programming tools often did not correctly implement many
programming language features. Then to get around the bugs that had been added,
non-standard, incompatible extensions are added. At Microsoft. for example,
this went as far as to be the policy of
[embrace, extend, and extinguish](https://en.wikipedia.org/wiki/Embrace%2C_extend%2C_and_extinguish).

Yoda Summary: Incompatibility &#8594; Suffering.

Then when it came time to use tools like C++ to create encapsulation libraries
to provide an easier-to-use programming model, these were often made to provide
only an imperfect rationalization of the programming task. This was so bad that
the Microsoft Foundation Class library was said to encapsulate Windows in the
same way a baby is encapsulated by a leaky diaper...

Yoda Summary: Ewe &#8594; Just ewe.

The confusion that was the result of all this greed based design would not
begin to fade until open sourced software rose in importance and popularity
spearheaded by systems such as Linux, and compilers like GCC. While open
source development is not a perfect cure all, its success is such that it has
forever changed the software landscape.

#### That Optimized Code Guy:

This point while not totally specific to the creation of graphical user
interfaces is strongly associated with it and the low level tools we used to
use to create them.

In almost every project I have worked on, I've run into that guy who insists on
optimizing the code _right away_. Invariably, when forced to deal with lower
level languages than are desired, it is necessary to build in a level of
abstraction in the code to allow for spec changes.

This is anathema to optimized-code-guy. Any and all deviations from the minimum
code path must be removed by them. This is fine until the specs change and all
the rigidly cross-linked code must be ripped up, rewired, and debugged, and
debugged, and debugged. Finally a patchwork of kludges, the code again works.

Connecting back to our first paragraph in this topic, nowhere do users make
more change requests than in the user interface. This is the part of the
application that the users must interact with on a constant basis. It is the
greatest point of pain for both users and programmers.

And then the customer adds "just these seven more changes please. ASAP please.
The process repeats and repeats and eventually the code is shredded so badly
that it needs to be reworked or thrown out and rewritten.

By this time, optimized code guy has moved on to wreck another project.

When should code be optimized then? Realistically, if the code works well
enough and optimization would take away abstraction and agility, the answer is
never. Seriously NEVER.

The exception would be the case where the performance is not adequate. Then the
code does not need optimization, it needs to be improved, usually by reworking
algorithms. Low level code tweaking is almost never enough to fix a performance
issues.

Summary: You can mow the lawn with a flame thrower, just don't expect good long
term results.

## Notes:
* All trademarks are the property of their respective owners.
